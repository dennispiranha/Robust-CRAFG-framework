{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ab09c2",
   "metadata": {},
   "source": [
    "# Important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069cadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "import math\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import random\n",
    "import gc\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7da2f",
   "metadata": {},
   "source": [
    "# Feature selection with MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_mutual_information(\n",
    "    df: pl.DataFrame, \n",
    "    col1: str, \n",
    "    col2: str, \n",
    "    num_bins: int = 10\n",
    "):\n",
    "    # Keep only required cols\n",
    "    df_proc = df.select([col1,col2])\n",
    "    df_proc = df_proc.filter((~pl.col(col1).is_nan())&(~pl.col(col1).is_infinite()))\n",
    "    n_unique = df_proc[col1].n_unique()\n",
    "    try:\n",
    "        if n_unique > num_bins:\n",
    "            # Quantile binning\n",
    "            df_proc = df_proc.with_columns(\n",
    "                pl.col(col1)\n",
    "                .qcut(num_bins, allow_duplicates=True)\n",
    "                .to_physical()  # turn into integer labels\n",
    "                .alias(col1)\n",
    "            )\n",
    "    except Exception:\n",
    "        # Fallback: equal-width bins\n",
    "        col_min = df_proc[col1].min()\n",
    "        col_max = df_proc[col1].max()\n",
    "        edges = np.linspace(col_min, col_max, num_bins + 1)\n",
    "        df_proc = df_proc.with_columns(\n",
    "            pl.col(col1)\n",
    "            .cut(edges)\n",
    "            .to_physical()\n",
    "            .alias(col1)\n",
    "        )\n",
    "\n",
    "    # Convert to numpy arrays for sklearn\n",
    "    x = df_proc.get_column(col1)\n",
    "    y = df_proc.get_column(col2)\n",
    "    return mutual_info_score(x,y)\n",
    "\n",
    "def create_mi_table(df_input,label,batch_size=1000):\n",
    "    # Exclude label column\n",
    "    total_features = [c for c in df_input.columns if c != label]\n",
    "    results = []\n",
    "    # Process in batches\n",
    "    for i in range(0, len(total_features), batch_size):\n",
    "        batch = total_features[i:i+batch_size]\n",
    "        batch_results = [\n",
    "            (feat,fast_mutual_information(df_input,feat,label,10)) for feat in batch\n",
    "        ]\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    # Build DataFrame once\n",
    "    df_mi = pl.DataFrame(results, schema=[\"Feature\",\"Mutual_Info_Score\"])\n",
    "    return df_mi.sort(\"Mutual_Info_Score\", descending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dd33b",
   "metadata": {},
   "source": [
    "# Feature generation function in First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a1fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, islice\n",
    "\n",
    "def generate_mono_feature_vectorized(df_pl, list_feature, index_feature):\n",
    "    # Ensure lazy mode\n",
    "    df_lazy = df_pl.lazy()\n",
    "    # Precompute mean and std for all features\n",
    "    stats = df_pl.select(\n",
    "        [pl.col(f).mean().alias(f\"{f}_mean\") for f in list_feature] +\n",
    "        [pl.col(f).std().alias(f\"{f}_std\") for f in list_feature]\n",
    "    ).to_dicts()[0]\n",
    "    meta_records = []\n",
    "    new_cols = []\n",
    "    index = index_feature\n",
    "    for feature in list_feature:\n",
    "        mean = stats[f\"{feature}_mean\"]\n",
    "        std = stats[f\"{feature}_std\"]\n",
    "        col_names_ops = [\n",
    "            (f\"p_{index}\",  pl.col(feature) ** 2,        \"Power\"),\n",
    "            (f\"sq_{index}\", pl.col(feature).sqrt(),      \"Square Root\"),\n",
    "            (f\"z_{index}\",  (pl.col(feature) - mean) / std, \"Z-Score\"),\n",
    "            (f\"lg_{index}\", pl.col(feature).log(base=2), \"Log base 2\"),\n",
    "            (f\"s_{index}\",  1 / (1 + (-1 * pl.col(feature)).exp()), \"Sigmoid\"),\n",
    "        ]\n",
    "        for name, expr, op in col_names_ops:\n",
    "            new_cols.append(expr.alias(name))\n",
    "            meta_records.append((name, feature, op))\n",
    "        index += 1\n",
    "    # Add all generated columns in one go\n",
    "    df_lazy = df_lazy.with_columns(new_cols)\n",
    "    # Collect the final dataset\n",
    "    df_result = df_lazy.collect()\n",
    "    # Create metadata DataFrame\n",
    "    df_meta = pl.DataFrame(meta_records, schema=[\"feature\", \"feature_L\", \"operation\"])\n",
    "    return df_result, df_meta, index\n",
    "\n",
    "def generate_binary_feature_fast_safe(df_pl,list_feature,index_feature,batch_size=200):\n",
    "    df_lazy = df_pl.lazy()\n",
    "    meta_records = []\n",
    "    index = index_feature\n",
    "    # Helper: yield batches of combinations\n",
    "    def batched_combinations(iterable, r, batch_size):\n",
    "        it = combinations(iterable, r)\n",
    "        while True:\n",
    "            batch = list(islice(it, batch_size))\n",
    "            if not batch:\n",
    "                break\n",
    "            yield batch\n",
    "    # Process in batches\n",
    "    for comb_batch in batched_combinations(list_feature, 2, batch_size):\n",
    "        new_cols = []\n",
    "        for comb in comb_batch:\n",
    "            ops = [\n",
    "                (f\"add_{index}\",   pl.col(comb[0]) + pl.col(comb[1]), \"Add\",      comb[0], comb[1]),\n",
    "                (f\"mul_{index}\",   pl.col(comb[0]) * pl.col(comb[1]), \"Multiply\", comb[0], comb[1]),\n",
    "                (f\"subt1_{index}\", pl.col(comb[0]) - pl.col(comb[1]), \"Subtract\", comb[0], comb[1]),\n",
    "                (f\"subt2_{index}\", pl.col(comb[1]) - pl.col(comb[0]), \"Subtract\", comb[1], comb[0]),\n",
    "                (f\"div1_{index}\",  pl.col(comb[0]) / pl.col(comb[1]), \"Divide\",   comb[0], comb[1]),\n",
    "                (f\"div2_{index}\",  pl.col(comb[1]) / pl.col(comb[0]), \"Divide\",   comb[1], comb[0]),\n",
    "            ]\n",
    "            for name, expr, op, left, right in ops:\n",
    "                new_cols.append(expr.alias(name))\n",
    "                meta_records.append((name, left, right, op))\n",
    "            index += 1\n",
    "        # Apply the batch of new columns\n",
    "        df_lazy = df_lazy.with_columns(new_cols)\n",
    "    df_result = df_lazy.collect()\n",
    "    df_meta = pl.DataFrame(meta_records, schema=[\"feature\", \"feature_L\", \"feature_R\", \"operation\"])\n",
    "    return df_result, df_meta, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afa0ca",
   "metadata": {},
   "source": [
    "# Dropping error feature after feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08672563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_data(df_pl_data,target,threshold_accept):\n",
    "    # threshold_accept : limit of portion which use to filter row of record\n",
    "    ### Drop row which contain empty value or useless value\n",
    "    total_feature = df_pl_data.columns\n",
    "    total_feature.remove(target)\n",
    "    df_pl = df_pl_data.select(total_feature)\n",
    "    criteria = round(df_pl.height*threshold_accept)\n",
    "    df_nan_counts = df_pl.select(pl.all().is_nan().sum())\n",
    "    df_filtered_nan = df_pl.select([col for col in df_nan_counts.columns if df_nan_counts.get_column(col)[0]<criteria])\n",
    "    df_inf_counts = df_filtered_nan.select(pl.all().is_infinite().sum())\n",
    "    df_filtered_inf = df_filtered_nan.select([col for col in df_inf_counts.columns if df_inf_counts.get_column(col)[0]<criteria])\n",
    "    df_zero_counts = df_filtered_inf.select(pl.all().eq(0).sum())\n",
    "    df_filtered_zero = df_filtered_inf.select([col for col in df_zero_counts.columns if df_zero_counts.get_column(col)[0]<criteria])\n",
    "    return df_pl_data.select(df_filtered_zero.columns+[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db6fb8",
   "metadata": {},
   "source": [
    "# Feature selection after feature generation (New Extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172f8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "def generate_meta_feature_fast(df_pl,list_feature):\n",
    "    # 1. Replace inf with 0 in a vectorized way\n",
    "    df_clean = df_pl.with_columns([\n",
    "        pl.when(pl.col(c).is_infinite()).then(0).otherwise(pl.col(c)).alias(c)\n",
    "        for c in df_pl.columns\n",
    "    ])\n",
    "    meta_records = []\n",
    "    # Precompute common stats in one go\n",
    "    means = df_clean.select([pl.col(c).mean().alias(c) for c in list_feature]).row(0)\n",
    "    stds = df_clean.select([pl.col(c).std().alias(c) for c in list_feature]).row(0)\n",
    "    missing_ratios = df_clean.select([\n",
    "        (pl.col(c).null_count() / pl.count()).alias(c) for c in list_feature\n",
    "    ]).row(0)\n",
    "    zero_ratios = df_clean.select([\n",
    "        ((pl.col(c) == 0).sum() / pl.count()).alias(c) for c in list_feature\n",
    "    ]).row(0)\n",
    "    cardinality_ratios = df_clean.select([\n",
    "        (pl.col(c).n_unique() / pl.count()).alias(c) for c in list_feature\n",
    "    ]).row(0)\n",
    "\n",
    "    # 2. Compute advanced stats per column (needs NumPy/SciPy)\n",
    "    for i, feat in enumerate(list_feature):\n",
    "        col_np = df_clean[feat].to_numpy()\n",
    "\n",
    "        # Drop NaNs for stats that can't handle them\n",
    "        col_np_clean = col_np[~np.isnan(col_np)]\n",
    "\n",
    "        meta_records.append((\n",
    "            feat,\n",
    "            means[i],\n",
    "            stds[i],\n",
    "            skew(col_np_clean) if len(col_np_clean) > 0 else np.nan,\n",
    "            kurtosis(col_np_clean) if len(col_np_clean) > 0 else np.nan,\n",
    "            entropy(np.histogram(col_np_clean, bins=\"auto\")[0], base=2) if len(col_np_clean) > 0 else np.nan,\n",
    "            missing_ratios[i],\n",
    "            zero_ratios[i],\n",
    "            cardinality_ratios[i],\n",
    "        ))\n",
    "\n",
    "    # 3. Build the final metadata DataFrame\n",
    "    df_feature_charactor = pl.DataFrame(\n",
    "        meta_records,\n",
    "        schema={\n",
    "            \"feature\": str,\n",
    "            \"mean\": float,\n",
    "            \"std\": float,\n",
    "            \"skewness\": float,\n",
    "            \"kurtosis\": float,\n",
    "            \"entropy\": float,\n",
    "            \"missing_ratio\": float,\n",
    "            \"zero_ratio\": float,\n",
    "            \"cardinality_ratio\": float\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_feature_charactor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709bf72",
   "metadata": {},
   "source": [
    "# Meta-learning of operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12d1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Build meta-feature ##########\n",
    "def entropy_histogram_polars(column: pl.Series, bins: int = 10) -> float:\n",
    "    values = column.drop_nulls().to_numpy()\n",
    "    if len(values) == 0:\n",
    "        return None  # avoid divide-by-zero or invalid entropy\n",
    "    hist, _ = np.histogram(values, bins=bins, density=False)\n",
    "    probs = hist / hist.sum()\n",
    "    probs = probs[probs > 0]  # remove zero entries to avoid log(0)\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "def summarize_polars_feature(column: pl.Series):\n",
    "    values = column.drop_nulls()\n",
    "    if values.is_empty():\n",
    "        return {\n",
    "            \"mean\": None,\n",
    "            \"std\": None,\n",
    "            \"skewness\": None,\n",
    "            \"kurtosis\": None,\n",
    "            \"entropy\": None,\n",
    "            \"missing_ratio\": column.null_count() / column.len(),\n",
    "            \"zero_ratio\": None,\n",
    "            \"cardinality_ratio\": None,\n",
    "        }\n",
    "\n",
    "    values_np = values.to_numpy()\n",
    "    counts = np.unique(values_np, return_counts=True)[1]\n",
    "    probs = counts / counts.sum()\n",
    "    probs = probs[probs > 0]\n",
    "    \n",
    "    return [values_np.mean(),values_np.std(ddof=1),skew(values_np,bias=False),kurtosis(values_np,bias=False),entropy_histogram_polars(column)\n",
    "            ,column.null_count()/column.len(),(values_np == 0).mean() if np.issubdtype(values_np.dtype, np.number) else None\n",
    "            ,len(np.unique(values_np))/column.len()]\n",
    "\n",
    "def generate_meta_feature(df_pl,list_feature):\n",
    "    df_feature_charactor = pl.DataFrame(schema={'feature':str,'mean':float,'std':float\n",
    "    ,'skewness':float,'kurtosis':float,'entropy':float\n",
    "    ,'missing_ratio':float,'zero_ratio':float,'cardinality_ratio':float})\n",
    "    df_pl = df_pl.with_columns([pl.col(col).apply(lambda x: 0 if x in [float('inf'),-float('inf')] else x) for col in df_pl.columns])\n",
    "    for feat in list_feature:\n",
    "        new_row = [feat]+summarize_polars_feature(df_pl[feat])\n",
    "        new_row_df = pl.DataFrame([new_row],schema=df_feature_charactor.schema)\n",
    "        df_feature_charactor = pl.concat([df_feature_charactor,new_row_df]) \n",
    "    return df_feature_charactor\n",
    "\n",
    "def calculate_centroid_meta_feature(df_meta_pair):\n",
    "    all_meta_feat = ['mean_sim','std_sim','skewness_sim','kurtosis_sim','entropy_sim'\n",
    "    ,'missing_ratio_sim','zero_ratio_sim','cardinality_ratio_sim']\n",
    "    df_select_feat = df_meta_pair[all_meta_feat+['operation']]\n",
    "    df_centroid_meta = pl.DataFrame(schema={'mean_sim':float,'std_sim':float\n",
    "    ,'skewness_sim':float,'kurtosis_sim':float,'entropy_sim':float\n",
    "    ,'missing_ratio_sim':float,'zero_ratio_sim':float,'cardinality_ratio_sim':float,'Operator':str})\n",
    "    for oper in [\"Add\",\"Subtract\",\"Multiply\",\"Divide\"]:\n",
    "        df_filter = df_select_feat.filter(pl.col('operation')==oper)\n",
    "        df_means = df_filter.mean()\n",
    "        df_means = df_means[all_meta_feat].with_columns([pl.lit(oper).alias(\"Operator\")])\n",
    "        df_centroid_meta = pl.concat([df_centroid_meta,df_means]) \n",
    "    return df_centroid_meta\n",
    "\n",
    "def calculate_centroid_meta_feature_fast(df_meta_pair):\n",
    "    meta_features = [\n",
    "        \"mean_sim\", \"std_sim\", \"skewness_sim\", \"kurtosis_sim\",\n",
    "        \"entropy_sim\", \"missing_ratio_sim\", \"zero_ratio_sim\", \"cardinality_ratio_sim\"\n",
    "    ]\n",
    "\n",
    "    # Group by 'operation', compute means\n",
    "    df_centroid_meta = (\n",
    "        df_meta_pair\n",
    "        .group_by(\"operation\")\n",
    "        .agg([pl.mean(f).alias(f) for f in meta_features])\n",
    "        .rename({\"operation\": \"Operator\"})\n",
    "    )\n",
    "\n",
    "    return df_centroid_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2058e",
   "metadata": {},
   "source": [
    "# Remove common pair of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b22cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_set_feature(list_feature,df_meta):\n",
    "    feature_unary,feature_binary = [],[]\n",
    "    df_filter_meta = df_meta.filter(pl.col('feature').is_in(list_feature))   \n",
    "    df_filter_binary = df_filter_meta.filter(pl.col('operation').is_in(['Add','Multiply','Subtract','Divide']))\n",
    "    feature_binary = df_filter_binary['feature'].to_list()\n",
    "    df_filter_unary = df_filter_meta.filter(pl.col('operation').is_in(['Power','Square Root','Z-Score','Log base 2','Sigmoid']))\n",
    "    feature_unary = df_filter_unary['feature'].to_list()\n",
    "    return feature_unary,feature_binary\n",
    "\n",
    "def get_all_unary_feature(list_feature,df_meta):\n",
    "    feature_unary = []\n",
    "    df_filter_meta = df_meta.filter(pl.col('feature').is_in(list_feature))\n",
    "    df_filter_unary = df_filter_meta.filter(pl.col('operation').is_in(['Power','Square Root','Z-Score','Log base 2','Sigmoid']))\n",
    "    feature_unary = df_filter_unary['feature'].to_list()\n",
    "    return feature_unary\n",
    "\n",
    "def root_of_feature(feature,df_solution):\n",
    "    temp_filter = df_solution.filter(pl.col('feature')==feature)\n",
    "    if temp_filter.height == 0:\n",
    "       return []\n",
    "    else:\n",
    "        level = temp_filter['level'][0]\n",
    "        if level==1:\n",
    "           df_temp = df_solution.filter(pl.col('feature')==feature)\n",
    "           ans_a = df_temp.select('feature_L')[0,0]\n",
    "           ans_b = df_temp.select('feature_R')[0,0]\n",
    "           root_feature = [ans_a,ans_b]\n",
    "           if 'None' in root_feature:\n",
    "              root_feature.remove('None')\n",
    "           return root_feature\n",
    "        else:\n",
    "             df_temp = df_solution.filter(pl.col('feature')==feature)\n",
    "             ans_a = df_temp.select('feature_L')[0,0]\n",
    "             ans_b = df_temp.select('feature_R')[0,0]\n",
    "             return root_of_feature(ans_a,df_solution)+root_of_feature(ans_b,df_solution)\n",
    "        \n",
    "def remove_redudant_root_feature(list_pair,df_solution): ### filter onliy pair of feature which no have intersect root of feature\n",
    "    filter_pair = [(pair[0],pair[1]) for pair in list_pair if len(list(set(root_of_feature(pair[0],df_solution))&set(root_of_feature(pair[1],df_solution))))<1]\n",
    "    return filter_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a14141",
   "metadata": {},
   "source": [
    "# Feature redundant removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Select only generated feature which is provides improve performance\n",
    "def select_feature_via_meta_fast(df_pl,label,meta_table,df_mi_table,current_level,num_top_feat):\n",
    "    # Get all features except label\n",
    "    all_feature = [f for f in df_pl.columns if f != label]\n",
    "    # Filter meta table to only current level & features present in df\n",
    "    df_filter = meta_table.filter((pl.col('level') == current_level)&(pl.col('feature').is_in(all_feature)))\n",
    "    # Separate unary and binary\n",
    "    unary_ops = [\"Power\", \"Z-Score\", \"Square Root\", \"Log base 2\", \"Sigmoid\"]\n",
    "    binary_ops = [\"Add\", \"Subtract\", \"Multiply\", \"Divide\"]\n",
    "    df_unary_feat = df_filter.filter(pl.col('operation').is_in(unary_ops))\n",
    "    df_binary_feat = df_filter.filter(pl.col('operation').is_in(binary_ops))\n",
    "    # ==== Unary Evaluation ====\n",
    "    df_map_unary = (\n",
    "        df_unary_feat\n",
    "        .join(df_mi_table, left_on=\"feature\", right_on=\"Feature\", how=\"left\")\n",
    "        .rename({\"Mutual_Info_Score\": \"MI_Feature\"})\n",
    "        .join(df_mi_table, left_on=\"feature_L\", right_on=\"Feature\", how=\"left\")\n",
    "        .rename({\"Mutual_Info_Score\": \"MI_Feature_L\"})\n",
    "        .filter(pl.col(\"MI_Feature\") > pl.col(\"MI_Feature_L\"))\n",
    "        .select(\"feature\",\"MI_Feature\")\n",
    "    )\n",
    "    select_unary_feat = df_map_unary[\"feature\"].to_list()\n",
    "    # ==== Binary Evaluation ====\n",
    "    df_map_binary = (\n",
    "        df_binary_feat\n",
    "        .join(df_mi_table, left_on=\"feature\", right_on=\"Feature\", how=\"left\")\n",
    "        .rename({\"Mutual_Info_Score\": \"MI_Feature\"})\n",
    "        .join(df_mi_table, left_on=\"feature_L\", right_on=\"Feature\", how=\"left\")\n",
    "        .rename({\"Mutual_Info_Score\": \"MI_Feature_L\"})\n",
    "        .join(df_mi_table, left_on=\"feature_R\", right_on=\"Feature\", how=\"left\")\n",
    "        .rename({\"Mutual_Info_Score\": \"MI_Feature_R\"})\n",
    "        .filter(\n",
    "            (pl.col(\"MI_Feature\") > pl.col(\"MI_Feature_L\")) |\n",
    "            (pl.col(\"MI_Feature\") > pl.col(\"MI_Feature_R\"))\n",
    "        )\n",
    "        .select(\"feature\",\"MI_Feature\")\n",
    "    )\n",
    "    select_binary_feat = df_map_binary[\"feature\"].to_list()\n",
    "    df_improve_feat = pl.concat([df_map_unary,df_map_binary])\n",
    "    df_improve_sort = df_improve_feat.sort(\"MI_Feature\",descending=True)\n",
    "    return df_improve_sort.head(num_top_feat)['feature'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32d64a",
   "metadata": {},
   "source": [
    "# Main+sub framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9ec1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validate_pair(df_input,list_original,label,df_meta): ##### Generate pair for built meta-feature for binary\n",
    "    all_feature = df_input.columns\n",
    "    all_feature.remove(label)\n",
    "    last_depth = df_meta['level'].max()\n",
    "    df_last_track = df_meta.filter(pl.col('level')==last_depth)\n",
    "    feat_unary,feat_binary = update_set_feature(all_feature,df_last_track) \n",
    "    feat_origi = list(set(all_feature)&set(list_original))\n",
    "    all_depth_unary = get_all_unary_feature(all_feature,df_meta)\n",
    "    #####################################################\n",
    "    self_pairs_binary = list(combinations(feat_binary,2))\n",
    "    self_pairs_unary = list(combinations(feat_unary,2))\n",
    "    across_origi_unary = list(product(feat_origi,feat_unary))\n",
    "    across_origi_binary = list(product(feat_origi,feat_binary))\n",
    "    across_unary_binary = list(product(all_depth_unary,feat_binary))\n",
    "    ########################################################\n",
    "    all_list_pair = [self_pairs_unary,self_pairs_binary,across_origi_unary,across_origi_binary,across_unary_binary]\n",
    "    validate_list_pair = []\n",
    "    for list_pair in all_list_pair:\n",
    "        if len(list_pair)>0 :\n",
    "           validate_pair = remove_redudant_root_feature(list_pair,df_meta)\n",
    "           if len(validate_pair)>0:\n",
    "              validate_list_pair.append(validate_pair)\n",
    "    merge_validate_pair = [item for sublist in validate_list_pair for item in sublist] # Concat list of pairs\n",
    "    return merge_validate_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40aaf1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_operator(df_meta_pair,df_represent):\n",
    "    df_score_operator = pl.DataFrame(schema={'feature_1':str,'feature_2':str\n",
    "    ,'cos_sim_add':float,'cos_sim_sub':float,'cos_sim_mul':float,'cos_sim_div':float})\n",
    "    all_meta_feat = ['mean_sim','std_sim','skewness_sim','kurtosis_sim','entropy_sim','missing_ratio_sim','zero_ratio_sim','cardinality_ratio_sim']\n",
    "    ######################################################\n",
    "    vector_add = np.array(df_represent.filter(pl.col('Operator')=='Add')[all_meta_feat].row(0))\n",
    "    vector_sub = np.array(df_represent.filter(pl.col('Operator')=='Subtract')[all_meta_feat].row(0))\n",
    "    vector_mul = np.array(df_represent.filter(pl.col('Operator')=='Multiply')[all_meta_feat].row(0))\n",
    "    vector_div = np.array(df_represent.filter(pl.col('Operator')=='Divide')[all_meta_feat].row(0))\n",
    "    ######################################################\n",
    "    for i in range(0,len(df_meta_pair)):\n",
    "        feat1 = df_meta_pair.row(i)[0]\n",
    "        feat2 = df_meta_pair.row(i)[1]\n",
    "        vector = np.array(df_meta_pair[all_meta_feat].row(i))\n",
    "        # Cosine similarity\n",
    "        sim_add = np.dot(vector,vector_add)/(np.linalg.norm(vector)*np.linalg.norm(vector_add))\n",
    "        sim_sub = np.dot(vector,vector_sub)/(np.linalg.norm(vector)*np.linalg.norm(vector_sub))\n",
    "        sim_mul = np.dot(vector,vector_mul)/(np.linalg.norm(vector)*np.linalg.norm(vector_mul))\n",
    "        sim_div = np.dot(vector,vector_div)/(np.linalg.norm(vector)*np.linalg.norm(vector_div))\n",
    "        new_row = [feat1,feat2,sim_add,sim_sub,sim_mul,sim_div]\n",
    "        new_row_df = pl.DataFrame([new_row],schema=df_score_operator.schema)\n",
    "        df_score_operator = pl.concat([df_score_operator,new_row_df]) \n",
    "    return df_score_operator\n",
    "\n",
    "def norm_cos_sim(x):\n",
    "    return (x+1)/2\n",
    "\n",
    "def roulette_wheel_operator(struct):\n",
    "    n_pick = 1 #<------- Change number Here!!!!!\n",
    "    sum_prob = struct['prob_add']+struct['prob_sub']+struct['prob_mul']+struct['prob_div']\n",
    "    weight_operator = {'Add':struct['prob_add'],'Subtract':struct['prob_sub']\n",
    "    ,'Multiply':struct['prob_mul'],'Divide':struct['prob_div']}\n",
    "    list_rec_operator = []\n",
    "    keys = list(weight_operator.keys())\n",
    "    weights = list(weight_operator.values())\n",
    "    probs = [w/sum_prob for w in weights]\n",
    "    while len(list_rec_operator)!=n_pick:\n",
    "          rec_operator = random.choices(keys,weights=probs,k=1)[0]\n",
    "          if rec_operator not in list_rec_operator:\n",
    "             list_rec_operator.append(rec_operator) \n",
    "    return list_rec_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab6cef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_meta_record(feat1,feat2,operator,index_feat):\n",
    "    df_meta_table = pl.DataFrame(schema={'feature':str,'feature_L':str,'feature_R':str,'operation':str}) \n",
    "    if operator=='Add':\n",
    "       new_row = ['add_'+str(index_feat),feat1,feat2,'Add']\n",
    "       new_row_df = pl.DataFrame([new_row],schema=df_meta_table.schema) \n",
    "    elif  operator=='Subtract':\n",
    "          new_row1 = ['subt1_'+str(index_feat),feat1,feat2,'Subtract']\n",
    "          new_row2 = ['subt2_'+str(index_feat),feat2,feat1,'Subtract']\n",
    "          new_row_df1 = pl.DataFrame([new_row1],schema=df_meta_table.schema)\n",
    "          new_row_df2 = pl.DataFrame([new_row2],schema=df_meta_table.schema)\n",
    "          new_row_df = pl.concat([new_row_df1,new_row_df2]) \n",
    "    elif  operator=='Multiply':\n",
    "          new_row = ['mul_'+str(index_feat),feat1,feat2,'Multiply']\n",
    "          new_row_df = pl.DataFrame([new_row],schema=df_meta_table.schema)\n",
    "    elif  operator=='Divide':\n",
    "          new_row1 = ['div1_'+str(index_feat),feat1,feat2,'Divide']\n",
    "          new_row2 = ['div2_'+str(index_feat),feat2,feat1,'Divide']\n",
    "          new_row_df1 = pl.DataFrame([new_row1],schema=df_meta_table.schema)\n",
    "          new_row_df2 = pl.DataFrame([new_row2],schema=df_meta_table.schema)\n",
    "          new_row_df = pl.concat([new_row_df1,new_row_df2])   \n",
    "    return new_row_df\n",
    "\n",
    "def rec_operator_to_meta(df_rec_operator, index_feature):\n",
    "    records = []  # store rows here\n",
    "    df_temp = df_rec_operator.select([\"feature_1\", \"feature_2\", \"best_operator\"])\n",
    "    for i in range(len(df_temp)):\n",
    "        feature_L, feature_R, list_operator = df_temp.row(i)\n",
    "        for oper in list_operator:\n",
    "            df_meta_record = build_meta_record(feature_L, feature_R, oper, index_feature)\n",
    "            index_feature += 1\n",
    "            # Instead of concat, just store as tuple/dict\n",
    "            records.append(df_meta_record.row(0))\n",
    "    # Build DataFrame once at the end\n",
    "    df_meta_table = pl.DataFrame(\n",
    "        records, \n",
    "        schema={\"feature\": str, \"feature_L\": str, \"feature_R\": str, \"operation\": str}\n",
    "    )\n",
    "    return df_meta_table, index_feature\n",
    "\n",
    "def generate_unary_from_meta(df_data,df_meta_unary):\n",
    "    expressions = []\n",
    "    for i in range(len(df_meta_unary)):\n",
    "        feature_name, feature_L, feature_R, operator = df_meta_unary.row(i)     \n",
    "        if operator == 'Power':\n",
    "           expr = (pl.col(feature_L)* pl.col(feature_L)).alias(feature_name)\n",
    "        elif operator == 'Square Root':\n",
    "             expr = pl.col(feature_L).sqrt().alias(feature_name)\n",
    "        elif operator == 'Z-Score':\n",
    "             mean = df_data[feature_L].mean()\n",
    "             std = df_data[feature_L].std()\n",
    "             expr = ((pl.col(feature_L) - mean)/std).alias(feature_name)\n",
    "        elif operator == 'Log base 2':\n",
    "             expr = pl.col(feature_L).log(base=2).alias(feature_name)\n",
    "        elif operator == 'Sigmoid':\n",
    "             expr = (1 / (1 + (-1 * pl.col(feature_L)).exp())).alias(feature_name)\n",
    "        else:\n",
    "            continue  # skip unknown operators   \n",
    "        expressions.append(expr)\n",
    "    # Add all new columns in a single call (faster, avoids repeated copies)\n",
    "    df_data = df_data.with_columns(expressions)\n",
    "    return df_data\n",
    "\n",
    "def generate_binary_from_meta(df_data,df_meta_bin):\n",
    "    expressions = []\n",
    "    for i in range(len(df_meta_bin)):\n",
    "        feature_name, feature_L, feature_R, operator = df_meta_bin.row(i)     \n",
    "        if operator == 'Add':\n",
    "            expr = (pl.col(feature_L) + pl.col(feature_R)).alias(feature_name)\n",
    "        elif operator == 'Subtract':\n",
    "            expr = (pl.col(feature_L) - pl.col(feature_R)).alias(feature_name)\n",
    "        elif operator == 'Multiply':\n",
    "            expr = (pl.col(feature_L) * pl.col(feature_R)).alias(feature_name)\n",
    "        elif operator == 'Divide':\n",
    "            expr = (pl.col(feature_L) / pl.col(feature_R)).alias(feature_name)\n",
    "        else:\n",
    "            continue  # skip unknown operators   \n",
    "        expressions.append(expr)\n",
    "    # Add all new columns in a single call (faster, avoids repeated copies)\n",
    "    df_data = df_data.with_columns(expressions)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc34c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_initial_centroid(list_operator):\n",
    "    df_centroid_meta = pl.DataFrame(schema={'mean_sim':float,'std_sim':float,'skewness_sim':float\n",
    "    ,'kurtosis_sim':float,'entropy_sim':float\n",
    "    ,'missing_ratio_sim':float,'zero_ratio_sim':float,'cardinality_ratio_sim':float,'Operator':str})\n",
    "    for oper in list_operator:\n",
    "        new_row = [0.5]*8+[oper]\n",
    "        new_row_df = pl.DataFrame([new_row],schema=df_centroid_meta.schema)\n",
    "        df_centroid_meta = pl.concat([df_centroid_meta,new_row_df]) \n",
    "        ### Clear unneed variables & Force force Python to free memory\n",
    "        del new_row_df\n",
    "        gc.collect()\n",
    "    return df_centroid_meta\n",
    "\n",
    "def update_centroid_fast(df_centroid_old,df_centroid_new,learning_rate):\n",
    "    \"\"\"\n",
    "    Update centroids with exponential moving average.\n",
    "    Keeps operators from old centroids if missing in new.\n",
    "    \"\"\"\n",
    "    meta_features = [\n",
    "        \"mean_sim\", \"std_sim\", \"skewness_sim\", \"kurtosis_sim\",\n",
    "        \"entropy_sim\", \"missing_ratio_sim\", \"zero_ratio_sim\", \"cardinality_ratio_sim\"\n",
    "    ]\n",
    "    # Outer join ensures we keep all operators\n",
    "    joined = df_centroid_old.join(df_centroid_new, on=\"Operator\", how=\"outer\", suffix=\"_new\")\n",
    "    # Build updated columns in one pass\n",
    "    updates = [\n",
    "        (pl.when(pl.col(f\"{feat}_new\").is_not_null())  # if new value exists\n",
    "         .then(pl.col(feat) + learning_rate * (pl.col(f\"{feat}_new\") - pl.col(feat)))\n",
    "         .otherwise(pl.col(feat))  # if no new value, keep old\n",
    "         .alias(feat))\n",
    "        for feat in meta_features\n",
    "    ]\n",
    "    result = joined.with_columns(updates).select(meta_features + [\"Operator\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c93387bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_meta_feature_centroid(\n",
    "    df_input: pl.DataFrame,\n",
    "    list_candidate_feat: list[str],\n",
    "    df_meta_binary: pl.DataFrame,\n",
    "    df_mi_for_map: pl.DataFrame,\n",
    "    limit_per_operation: int\n",
    "):\n",
    "    # Use Lazy for better memory management\n",
    "    df_meta_bin_filter = (\n",
    "        df_meta_binary.lazy()\n",
    "        .filter(pl.col(\"feature\").is_in(list_candidate_feat))\n",
    "    )\n",
    "\n",
    "    # Join with MI scores\n",
    "    df_map_1 = df_meta_bin_filter.join(\n",
    "        df_mi_for_map.lazy(),\n",
    "        left_on=\"feature\", right_on=\"Feature\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Select only top N per operation (cuts down data early)\n",
    "    df_top_sorted = (\n",
    "        df_map_1\n",
    "        .sort(\"Mutual_Info_Score\", descending=True)\n",
    "        .group_by(\"operation\")\n",
    "        .head(limit_per_operation)\n",
    "    ).collect(streaming=True)\n",
    "\n",
    "    # Avoid huge Python lists — keep in Polars\n",
    "    df_pair_feat = df_top_sorted.select([\"feature_L\", \"feature_R\"]).unique()\n",
    "    pair_feature = [row for row in df_pair_feat.iter_rows()]\n",
    "\n",
    "    # Extract unique features directly in Polars\n",
    "    unique_sub_feat = (\n",
    "        df_top_sorted.select([\"feature_L\", \"feature_R\"])\n",
    "        .melt()\n",
    "        .select(\"value\")\n",
    "        .unique()\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    # Generate meta features\n",
    "    df_meta_feature = generate_meta_feature_fast(df_input, unique_sub_feat)\n",
    "    df_meta_pair_feature = generate_pair_meta_feature_fast(df_meta_feature, pair_feature)\n",
    "\n",
    "    # Second join (also lazy + streaming)\n",
    "    df_map_2 = (\n",
    "        df_meta_pair_feature.lazy().join(\n",
    "            df_map_1,\n",
    "            left_on=[\"feature_1\", \"feature_2\"],\n",
    "            right_on=[\"feature_L\", \"feature_R\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_top_sorted2 = (\n",
    "        df_map_2\n",
    "        .sort(\"Mutual_Info_Score\", descending=True)\n",
    "        .group_by(\"operation\")\n",
    "        .head(limit_per_operation)\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "    return df_meta_feature, calculate_centroid_meta_feature_fast(df_top_sorted2)\n",
    "\n",
    "### Must assign existing meta-feature\n",
    "def built_meta_feature_centroid_second(\n",
    "    df_input: pl.DataFrame,\n",
    "    list_candidate_feat: list[str],\n",
    "    df_meta_feature: pl.DataFrame,\n",
    "    df_meta_binary: pl.DataFrame,\n",
    "    df_mi_for_map: pl.DataFrame,\n",
    "    limit_per_operation: int\n",
    "):\n",
    "    # Step 1: Filter only candidate features\n",
    "    df_meta_bin_filter = (\n",
    "        df_meta_binary.lazy()\n",
    "        .filter(pl.col(\"feature\").is_in(list_candidate_feat))\n",
    "    )\n",
    "\n",
    "    # Step 2: Join with MI scores\n",
    "    df_map_1 = df_meta_bin_filter.join(\n",
    "        df_mi_for_map.lazy(),\n",
    "        left_on=\"feature\", right_on=\"Feature\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Get top-N per operation\n",
    "    df_top_sorted = (\n",
    "        df_map_1\n",
    "        .sort(\"Mutual_Info_Score\", descending=True)\n",
    "        .group_by(\"operation\")\n",
    "        .head(limit_per_operation)\n",
    "        .collect(streaming=True)  # <-- streaming reduces memory\n",
    "    )\n",
    "\n",
    "    # Step 4: Extract unique sub-features (avoid two big .to_list())\n",
    "    unique_sub_feat = (\n",
    "        df_top_sorted.select([\"feature_L\", \"feature_R\"])\n",
    "        .melt()\n",
    "        .select(\"value\")\n",
    "        .unique()\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    # Step 5: Extract unique feature pairs (still small after top-N)\n",
    "    pair_feature = (\n",
    "        df_top_sorted.select([\"feature_L\", \"feature_R\"])\n",
    "        .unique()\n",
    "        .iter_rows()\n",
    "    )\n",
    "\n",
    "    # Step 6: Generate new meta features for sub-features\n",
    "    df_meta_feature_new = generate_meta_feature_fast(df_input,unique_sub_feat)\n",
    "\n",
    "    # Append (avoid full duplication → drop duplicates if needed)\n",
    "    df_meta_feature = pl.concat(\n",
    "        [df_meta_feature, df_meta_feature_new],\n",
    "        how=\"vertical_relaxed\"  # more efficient than default concat\n",
    "    ).unique(maintain_order=True)\n",
    "\n",
    "    # Step 7: Generate pair meta features\n",
    "    df_meta_pair_feature = generate_pair_meta_feature_fast(df_meta_feature, pair_feature)\n",
    "\n",
    "    # Step 8: Second join + top-N per operation\n",
    "    df_map_2 = (\n",
    "        df_meta_pair_feature.lazy().join(\n",
    "            df_map_1,\n",
    "            left_on=[\"feature_1\", \"feature_2\"],\n",
    "            right_on=[\"feature_L\", \"feature_R\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_top_sorted2 = (\n",
    "        df_map_2\n",
    "        .sort(\"Mutual_Info_Score\", descending=True)\n",
    "        .group_by(\"operation\")\n",
    "        .head(limit_per_operation)\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "    # Return updated centroid\n",
    "    return df_meta_feature, calculate_centroid_meta_feature_fast(df_top_sorted2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2781d37",
   "metadata": {},
   "source": [
    "# Sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0550dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_binary(df: pl.DataFrame, label: str, n_sub: int, seed: int = 42) -> pl.DataFrame:\n",
    "    \"\"\"Stratified sample preserving class balance.\"\"\"\n",
    "    # class proportions\n",
    "    counts = df.group_by(label).len().sort(label)\n",
    "    N = counts[\"len\"].sum()\n",
    "    # allocate per class\n",
    "    alloc = counts.with_columns(\n",
    "        (pl.col(\"len\") / N * n_sub).round(0).cast(pl.Int64).alias(\"take\")\n",
    "    )\n",
    "    parts = []\n",
    "    for cls, take in zip(alloc[label].to_list(), alloc[\"take\"].to_list()):\n",
    "        part = df.filter(pl.col(label) == cls).sample(n=int(max(0, take)),\n",
    "                                                      with_replacement=False,\n",
    "                                                      shuffle=True, seed=seed)\n",
    "        parts.append(part)\n",
    "    # adjust if rounding lost a few rows\n",
    "    out = pl.concat(parts)\n",
    "    deficit = n_sub - out.height\n",
    "    if deficit > 0:\n",
    "        remaining = df.filter(~pl.col(\"row_nr\").is_in(out.with_row_count(\"row_nr\")[\"row_nr\"])) if \"row_nr\" in df.columns else df\n",
    "        out = pl.concat([out, remaining.sample(n=deficit, with_replacement=False, shuffle=True, seed=seed+1)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2100d",
   "metadata": {},
   "source": [
    "# Main sub-algorithm of Robust-CRAFG framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f544a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result_generation:\n",
    "    def __init__(self,df_result,df_meta,df_meta_feat,df_mi,df_centroid,lasted_index,_iterate):\n",
    "        self.df_generated = df_result\n",
    "        self.df_meta_data = df_meta\n",
    "        self.df_meta_feature = df_meta_feat\n",
    "        self.df_mi_table = df_mi \n",
    "        self.df_meta_centroid = df_centroid\n",
    "        self.iterate = _iterate\n",
    "        self.last_index = lasted_index\n",
    "\n",
    "def first_feature_generation(df_input,list_filter_feat,num_select_feat,label,mi_table):\n",
    "    #### Generate feature both of unary and binary ####\n",
    "    df_gen_unary,df_meta_unary,last_index = generate_mono_feature_vectorized(df_input,list_filter_feat,1)\n",
    "    df_gen_binary,df_meta_binary,last_index = generate_binary_feature_fast_safe(df_gen_unary,list_filter_feat,last_index+1,200)\n",
    "    del df_gen_unary; gc.collect()\n",
    "    #### Drop row that contains error value ##########\n",
    "    df_drop_error = drop_missing_data(df_gen_binary,label,0.3) #<------- Can change percent dropping error feature\n",
    "    list_feature_drop_error = [f for f in df_drop_error.columns if f!=label]\n",
    "    del df_drop_error; gc.collect()\n",
    "    #### Select only unary&binary feature which survive from dropping error\n",
    "    unary_feat = list(set(df_meta_unary['feature'].to_list())&(set(list_feature_drop_error)))\n",
    "    binary_feat = list(set(df_meta_binary['feature'].to_list())&(set(list_feature_drop_error)))\n",
    "    #### Create MI table\n",
    "    df_mi_unary = create_mi_table(df_gen_binary[unary_feat+[label]],label,1000)\n",
    "    df_mi_binary = create_mi_table(df_gen_binary[binary_feat+[label]],label,1000)\n",
    "    df_mi_table_new = pl.concat([mi_table,df_mi_unary,df_mi_binary])\n",
    "    del df_mi_unary; gc.collect()\n",
    "    #### Create meta-table\n",
    "    df_meta_unary2 = df_meta_unary.with_columns(pl.lit('None').alias('feature_R'))\n",
    "    df_meta_unary2 = df_meta_unary2.select(['feature','feature_L','feature_R','operation'])\n",
    "    df_meta_combine = pl.concat([df_meta_unary2,df_meta_binary])\n",
    "    del df_meta_unary2; gc.collect()\n",
    "    df_meta_combine = df_meta_combine.with_columns(pl.lit(1).alias('level'))\n",
    "    #### Select only unary&binary feature which provide bettet classfication performance  \n",
    "    cols = unary_feat+binary_feat+[label]\n",
    "    df_temp = df_gen_binary.select(cols)\n",
    "    improved_feat = select_feature_via_meta_fast(df_temp,label,df_meta_combine,df_mi_table_new,1,num_select_feat)\n",
    "    del df_temp; gc.collect()\n",
    "    #### Build meta-feature of binary feature (using between two feature) ##############\n",
    "    limit_per_operation = len(df_input.columns)-1\n",
    "    df_meta_feature,df_centroid_meta = built_meta_feature_centroid(df_gen_binary,improved_feat,df_meta_binary,df_mi_table_new,limit_per_operation)\n",
    "    list_bin_oper = [\"Add\",\"Multiply\",\"Subtract\",\"Divide\"]\n",
    "    df_initial_centroid = gen_initial_centroid(list_bin_oper)\n",
    "    df_update_centroid = update_centroid_fast(df_initial_centroid,df_centroid_meta,1)\n",
    "    final_cols = list_filter_feat+improved_feat+[label]\n",
    "    result_obj = Result_generation(df_gen_binary.select(final_cols),df_meta_combine,df_meta_feature,df_mi_table_new,df_update_centroid,last_index,1)\n",
    "    return result_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96dab457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sequent_feature_generation(obj_result,list_filter_origi_feat,num_select_feat,label):\n",
    "    all_feature = [f for f in obj_result.df_generated.columns if f!=label]\n",
    "    df_last_depth = obj_result.df_meta_data.filter(pl.col('level')==(obj_result.iterate))\n",
    "    feat_unary,feat_binary = update_set_feature(all_feature,df_last_depth)\n",
    "    #### Generate unary feature via exhaustive method\n",
    "    df_gen_unary,df_meta_unary,last_index = generate_mono_feature_vectorized(\n",
    "    obj_result.df_generated,feat_unary+feat_binary,obj_result.last_index+1)\n",
    "    df_meta_unary2 = df_meta_unary.with_columns(pl.lit('None').alias('feature_R'))\n",
    "    df_meta_unary2 = df_meta_unary2.select(['feature','feature_L','feature_R','operation'])\n",
    "    validate_list = generate_validate_pair(obj_result.df_generated,list_filter_origi_feat,label,obj_result.df_meta_data)\n",
    "    #######################\n",
    "    set_feature = set()\n",
    "    for pair in validate_list:\n",
    "        temp_set = set(pair)\n",
    "        set_feature = set_feature.union(temp_set)\n",
    "    unique_feat = list(set_feature)\n",
    "    ########################\n",
    "    feat_in_meta = obj_result.df_meta_feature.get_column('feature').to_list()\n",
    "    other_feat_in_meta = [f for f in unique_feat if f not in feat_in_meta]\n",
    "    df_meta_feature_new = generate_meta_feature_fast(obj_result.df_generated,other_feat_in_meta)\n",
    "    meta_feature_merged = pl.concat([obj_result.df_meta_feature,df_meta_feature_new])\n",
    "    df_meta_pair_feature = generate_pair_meta_feature_fast(meta_feature_merged,validate_list)\n",
    "    ### Select top 2 operator for pair of feature\n",
    "    df_choose_operator = calculate_score_operator(df_meta_pair_feature,obj_result.df_meta_centroid)\n",
    "    df_choose_operator = df_choose_operator.with_columns(pl.col('cos_sim_add').map_elements(norm_cos_sim).alias('prob_add'))\\\n",
    "    .with_columns(pl.col('cos_sim_sub').map_elements(norm_cos_sim).alias('prob_sub'))\\\n",
    "    .with_columns(pl.col('cos_sim_mul').map_elements(norm_cos_sim).alias('prob_mul'))\\\n",
    "    .with_columns(pl.col('cos_sim_div').map_elements(norm_cos_sim).alias('prob_div'))\\\n",
    "    .with_columns(pl.struct(['prob_add','prob_sub','prob_mul','prob_div']).map_elements(roulette_wheel_operator).alias('best_operator'))\n",
    "    #### Build meta table from recommend operator\n",
    "    df_meta_binary,last_index = rec_operator_to_meta(df_choose_operator,last_index+1)\n",
    "    #### Generate binary feature from Built meta table\n",
    "    df_gen_binary = generate_binary_from_meta(df_gen_unary,df_meta_binary)\n",
    "    del df_gen_unary; gc.collect()\n",
    "    df_meta_update = pl.concat([df_meta_unary2,df_meta_binary])\n",
    "    df_meta_update = df_meta_update.with_columns(pl.lit(obj_result.iterate+1).alias('level'))\n",
    "    df_meta_table = pl.concat([obj_result.df_meta_data,df_meta_update])\n",
    "    #### Drop row that contains error value ##########\n",
    "    df_drop_error = drop_missing_data(df_gen_binary,label,0.3)\n",
    "    list_feature_drop_error = [f for f in df_drop_error.columns if f!=label]\n",
    "    del df_drop_error; gc.collect()\n",
    "    #### Select only unary&binary feature which survive from dropping error\n",
    "    unary_feat = list(set(df_meta_unary['feature'].to_list())&(set(list_feature_drop_error)))\n",
    "    binary_feat = list(set(df_meta_binary['feature'].to_list())&(set(list_feature_drop_error)))\n",
    "    #### Create MI table \n",
    "    df_mi_unary = create_mi_table(df_gen_binary[unary_feat+[label]],label,1000)\n",
    "    df_mi_binary = create_mi_table(df_gen_binary[binary_feat+[label]],label,1000)\n",
    "    df_mi_table_new = pl.concat([obj_result.df_mi_table,df_mi_unary,df_mi_binary])\n",
    "    del df_mi_unary; gc.collect()\n",
    "    #### Select only unary&binary feature which provide bettet classfication performance  \n",
    "    cols = unary_feat+binary_feat+[label]\n",
    "    df_temp = df_gen_binary.select(cols)\n",
    "    improved_feat = select_feature_via_meta_fast(df_temp,label,df_meta_table,df_mi_table_new,obj_result.iterate+1,num_select_feat)\n",
    "    del df_temp; gc.collect()\n",
    "    #### Calculate new Centroid of meta-feature \n",
    "    limit_per_operation = len(obj_result.df_generated.columns)-1\n",
    "    df_meta_feature,df_centroid_meta_new = built_meta_feature_centroid_second(df_gen_binary,improved_feat,meta_feature_merged,df_meta_binary,df_mi_table_new,limit_per_operation)\n",
    "    #### Update Centroid of meta-feature \n",
    "    df_centroid_meta = update_centroid_fast(obj_result.df_meta_centroid,df_centroid_meta_new,1/(obj_result.iterate+1))\n",
    "    final_cols = list_filter_origi_feat+improved_feat+[label]\n",
    "    result_obj = Result_generation(df_gen_binary.select(final_cols),df_meta_table,df_meta_feature,df_mi_table_new,df_centroid_meta,last_index,obj_result.iterate+1)\n",
    "    return result_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f04be022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_info_full_data(df_original,label,dict_sub_generated,meta_table):\n",
    "    unary_ops = [\"Power\", \"Z-Score\", \"Square Root\", \"Log base 2\", \"Sigmoid\"]\n",
    "    binary_ops = [\"Add\", \"Subtract\", \"Multiply\", \"Divide\"]\n",
    "    df_full = df_original.clone()\n",
    "    key_iterate = dict_sub_generated.keys()\n",
    "    dict_data_generated = {}\n",
    "    expressions = []\n",
    "    total_feature = set()\n",
    "    ##### Generate feature follow meta-table into full dataset\n",
    "    for iterate in key_iterate:\n",
    "        features = [f for f in dict_sub_generated[iterate].columns if f!=label]\n",
    "        df_filter_meta = meta_table.filter((pl.col('level')==iterate)&(pl.col('feature').is_in(features)))\n",
    "        df_filter_meta = df_filter_meta.select(['feature','feature_L','feature_R','operation'])\n",
    "        meta_unary = df_filter_meta.filter(pl.col('operation').is_in(unary_ops))\n",
    "        meta_binary = df_filter_meta.filter(pl.col('operation').is_in(binary_ops))\n",
    "        if meta_unary.height>0:\n",
    "           df_full = generate_unary_from_meta(df_full,meta_unary) \n",
    "        if meta_binary.height>0:\n",
    "           df_full = generate_binary_from_meta(df_full,meta_binary)\n",
    "        df_gen = df_full.select(features+[label])\n",
    "        total_feature = total_feature.union(set(features))\n",
    "        dict_data_generated[iterate] = df_gen\n",
    "    ##### Calculate MI table of all feature on entire full dataset\n",
    "    df_mi_table = create_mi_table(df_full,label,batch_size=1000)   \n",
    "    return dict_data_generated,df_mi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fa06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import polars as pl\n",
    "\n",
    "def rebalance_data(df_input,label:str):\n",
    "    cols_name = [i for i in df_input.columns if i!=label]\n",
    "    # Split into X, y (as pandas for imblearn)\n",
    "    X = df_input.drop(label).to_pandas()\n",
    "    y = df_input.select(label).to_pandas().squeeze()\n",
    "\n",
    "    # CNN undersampling\n",
    "    cnn = CondensedNearestNeighbour(n_neighbors=1, random_state=42)\n",
    "    X_cnn, y_cnn = cnn.fit_resample(X,y)\n",
    "\n",
    "    # Count classes\n",
    "    class_counts = Counter(y_cnn)\n",
    "    minority_class, minority_count = min(class_counts.items(),key=lambda x: x[1])\n",
    "    majority_class, majority_count = max(class_counts.items(),key=lambda x: x[1])\n",
    "\n",
    "    # Ratio of minority\n",
    "    ratio_minority = minority_count / (minority_count + majority_count)\n",
    "\n",
    "    if 0.45 <= ratio_minority <= 0.55:\n",
    "        # Convert back to Polars\n",
    "        X_cnn_pl = pl.DataFrame(X_cnn,schema=cols_name)\n",
    "        y_cnn_pl = pl.Series(label, y_cnn)\n",
    "        df_cnn = pl.concat([X_cnn_pl, y_cnn_pl.to_frame()], how=\"horizontal\")\n",
    "        return df_cnn\n",
    "    else:\n",
    "        # Apply SMOTE on CNN result\n",
    "        smote = SMOTE(sampling_strategy=\"auto\",random_state=42, k_neighbors=5)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X_cnn,y_cnn)\n",
    "        # Convert back to Polars\n",
    "        X_balanced_pl = pl.DataFrame(X_balanced, schema=cols_name)\n",
    "        y_balanced_pl = pl.Series(label, y_balanced)\n",
    "        df_balanced = pl.concat([X_balanced_pl, y_balanced_pl.to_frame()],how=\"horizontal\")\n",
    "        return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a903eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_crafg(\n",
    "    data_pl: pl.DataFrame,\n",
    "    label: str,\n",
    "    per_filter: float,\n",
    "    skip_first_select: str = \"N\",\n",
    "    iteration: int = 1,\n",
    "    skip_sub_samp: str = \"Y\",\n",
    "    ratio_sub_samp: float = 0.25,\n",
    "    handle_imbalance: str = \"N\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pl : polars.DataFrame\n",
    "        Input dataset containing both features and the target column.\n",
    "    label : str\n",
    "        Name of the target column.\n",
    "    per_filter : float\n",
    "        Fraction (0 < per_filter ≤ 1) of features to retain after each MI-based\n",
    "        filtering step.\n",
    "    skip_first_select : {\"Y\", \"N\"}, default=\"N\"\n",
    "        If \"Y\", skip the initial MI filtering step. If \"N\", perform filtering\n",
    "        in the first iteration.\n",
    "    iteration : int, default=1\n",
    "        Number of feature generation and selection iterations to perform.\n",
    "    skip_sub_samp : {\"Y\", \"N\"}, default=\"Y\"\n",
    "        If \"Y\", skip subsampling. If \"N\", perform stratified subsampling in each\n",
    "        iteration.\n",
    "    ratio_sub_samp : float, default=1.0\n",
    "        Fraction (0 < ratio_sub_samp ≤ 1) of the dataset to use during subsampling.\n",
    "        Ignored if `skip_sub_samp=\"Y\"`.\n",
    "    handle_imbalance : {\"Y\", \"N\"}, default=\"N\"\n",
    "        If \"Y\", apply imbalance handling (undersampling/oversampling) in the\n",
    "        initial iteration. If \"N\", do not rebalance.\n",
    "    \"\"\"\n",
    "    dict_generated = {}\n",
    "    if skip_sub_samp == 'N':\n",
    "       n_sub = ratio_sub_samp*data_pl.height\n",
    "       df_input = stratified_sample_binary(data_pl,label,n_sub,41)\n",
    "       if handle_imbalance == 'Y':\n",
    "          df_input = rebalance_data(df_input,label) \n",
    "    else:\n",
    "         if handle_imbalance == 'Y':\n",
    "            df_input = rebalance_data(data_pl,label) \n",
    "         else:\n",
    "            df_input = data_pl.clone() \n",
    "    # Safe way to exclude target column\n",
    "    feature_origi = [c for c in data_pl.columns if c != label]\n",
    "    total_num_feature = len(feature_origi)\n",
    "    num_filter_feature = round(per_filter * total_num_feature)\n",
    "    #### Step 1: First feature filtering (optional) ####\n",
    "    df_mi_table = create_mi_table(df_input,label,1000)\n",
    "    if skip_first_select == \"N\":\n",
    "       filter_feature = (df_mi_table.head(num_filter_feature).get_column(\"Feature\").to_list())\n",
    "    else:\n",
    "        filter_feature = feature_origi\n",
    "    ####################################################\n",
    "    obj_generated = first_feature_generation(df_input,filter_feature,num_filter_feature,label,df_mi_table)\n",
    "    dict_generated[1] = obj_generated.df_generated\n",
    "    if iteration==1:\n",
    "       pass\n",
    "    else:\n",
    "        ############# Next Iteration #############\n",
    "        for iterate in range(2,iteration+1): \n",
    "            obj_generated = sub_sequent_feature_generation(obj_generated,filter_feature,num_filter_feature,label)\n",
    "            dict_generated[iterate] = obj_generated.df_generated\n",
    "        ##########################################\n",
    "    if skip_sub_samp == 'Y' and handle_imbalance == 'N':\n",
    "       return dict_generated,obj_generated\n",
    "    else:\n",
    "         meta_table = obj_generated.df_meta_data\n",
    "         dict_full_generated,df_mi_table_full = mapping_info_full_data(data_pl,label,dict_generated,meta_table)\n",
    "         obj_generated_full = Result_generation(\n",
    "                             dict_full_generated[iteration]\n",
    "                            ,obj_generated.df_meta_data\n",
    "                            ,obj_generated.df_meta_feature\n",
    "                            ,df_mi_table_full\n",
    "                            ,obj_generated.df_meta_centroid\n",
    "                            ,obj_generated.last_index\n",
    "                            ,iteration)\n",
    "         return dict_full_generated,obj_generated_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
